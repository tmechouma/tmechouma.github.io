# Toufik Mechouma - PhD  in Cognitive Computing 
Cloud MLOps | Fine-tune LLMs| Cloud Automation | Datascience | Vision & Language LLMs | Multimodal LLMs | NeuroSymbolic AI.

**I help companies elevate their business to the next level by harnessing the power of AI, enabling them to stay competitive and thrive in an increasingly data-driven world** :grinning:

## About Me :
<div align="justify">
A highly skilled NLP expert and passionate AI researcher, driven by a mission to design and develop state-of-the-art multimodal large language models (LLMs) that transcend symbolic surface forms to semotize concepts—capturing meaning through grounded, perceptual representations. My research vision lies in bridging the gap between linguistic symbols and real-world semantics by fusing vision and language at the conceptual level, creating truly intelligent systems that understand, reason, and interact like humans.

I integrate cutting-edge technologies including Transformers, LLMs variants, and powerful toolkits like LangChain, Semantic Kernel, Haystack, LiteLLM, and LlamaIndex to build retrieval-augmented generation (RAG) systems and intelligent agents capable of real-world problem solving. My multimodal pipeline involves CLIP, BLIP-2, Kosmos-2, and Flamingo, tightly woven with language processing layers to enable deep contextual understanding across text, image, and video modalities.

With hands-on expertise in vector databases (FAISS, Pinecone, Weaviate, Milvus), I implement semantic indexing and grounded concept linking using external knowledge bases and latent spaces derived from vision models—an approach that redefines the nature of understanding and inference in AI.

Proficient in Python, Spark, SQL, and modern DevOps/MLOps practices (Docker, GitHub Actions, MLflow, Terraform, Kubernetes), I deploy scalable, production-ready AI services in Azure, GCP, and AWS cloud ecosystems. My architecture philosophy embraces CI/CD, data-centric AI, and research-to-deployment alignment, ensuring that cutting-edge insights seamlessly translate into enterprise-grade solutions.

An enthusiastic collaborator and lifelong learner, I thrive at the intersection of research and application, constantly pushing the frontiers of multimodal cognition, grounded meaning, and neurosymbolic AI. I’m on a mission to contribute to the next generation of LLMs that can see, reason, and communicate with conceptual depth—not just linguistic fluency.
</div>

### Core Competencies
- 	**LLMs**: BERT, GPT-3, T5, XLNet, Gemini, LLaMA, RoBERTa, ALBERT, Electra, BioBERT, lingBERT, SCABERT, VLG-BERT.
- 	**MULTIMODAL LLMs**: GPT 4 Turbo, DALL·E, Gemini 1.5, Claude 3, LLaVA 
- 	**NLP Data Processing**: NLTK, spaCy, TextBlob, Gensim, OpenNLP, StanfordNLP
- 	**LLMs Modeling Techniques**: Sequence-to-Sequence (Seq2Seq), Attention Mechanisms, Masked Language Models, Pre-trained Models, Transfer Learning, Meta-learning
- 	**Vector Databases**: FAISS, Pinecone, Weaviate, Milvus
- 	**Retrieval-Augmented Generation (RAG)**: LangChain, Semantic Kernel, Haystack, LiteLLM, LlamaIndex
- 	**MS Azure Machine Learning**: ML Studio, ML Pipelines, ML SDK, Datasets & Data Stores, Model Deployment (ACI, AKS)
- 	**Azure Cognitive Services**: Text Analytics API, Language Understanding (LUIS), Real-time text translation, QnA Maker, Computer Vision API, Anomaly Detector API
- 	**Applications**: Text Generation, Named Entity Recognition (NER), Machine Translation, Sentiment Analysis, Question Answering, Summarization
- 	**Cloud & API Management**: Azure (ML, Storage, API Management), GCP, FastAPI  
- 	**DevOps**: GitHub, GitLab, Jenkins, GitHub Actions, GitLab CI/CD, Docker, Kubernetes
- 	**Machine Learning Libraries**: Keras, Pytorch, Sklearn, TensorFlow, Hugging Face Transformers, AllenNLP
- 	**Optimization and Attention Mechanisms**: Multi-Head Attention, Self-Attention, Transformers
- 	**Programming Languages**: Python, Java
- 	**Data Management**: SQL, SQL Server, Oracle, Oracle SQL Developper, NoSQL, MongoDB, Neo4j, Apache Spark.
## Additional Competencies
- **Cloud Networking**: BRIDGE, IPVLAN, MACVLAN, OVERLAY on DOCKER, DOCKER SWARM and KUBERNETES Nodes Network Conception
- **Networking**: Advanced Level in Cisco Routing and Switching
- **Security**: i am passionate about Kali linux, exploits and penetration testing
- **Business Intelligence**: MS SSRS, SSIS, SSAS Dashboards to help on decision making
- **Virtualization**: Vmware ESXI,vCenter, PROXMOX
- **Agility**: SCRUM MASTER PSM I
- **Machine Learning for cybersecurity**: Integration of ML algorithms for Cybersecurity and information systems security.


---
### Research Interests
- **Natural Language Processing (NLP)**: Developing advanced methods for deeper encoding of word and sentence meanings to enhance understanding and interpretation.
- **Multimodal Learning**: Combining vision, language, and other sensory inputs to build a holistic AI.
- **Cognitive Computing (Perception)**: Leveraging deep neural networks as perception modules for interpreting multimodal inputs.
- **Cognitive Computing (Memory)**: Modeling semantic and episodic memory as a complementary component to Retrieval-Augmented Generation (RAG) in multimodal large language models (LLMs).
- **Cognitive Computing (Representation)**: Toward the generation of neural ontologies for enriched knowledge representation.
- **Cognitive Computing (Reasoning**): Toward abstracting neural ontology-based knowledge representations into formal concepts to enable logical reasoning.
- **AI Topdown and Bottomup AI approaches**: Merging top-down and bottom-up AI approaches to achieve enhanced interpretability and deeper insights.
- **Causal Inference & Causality**: Applying causal inference techniques to real-world industrial challenges, such as failure detection and root cause analysis.
---


## Research Papers & Publications

1. **[VLG-BERT: Towards Better Interpretability in LLMs through Visual and
Linguistic Grounding](https://www.nlp4dh.com/nlp4dh-2025)** 
   Published in [**NLP4DH 2025** - **NAACL 2025**], May 4th 2025,**Albuquerque, USA**.
   <div align="justify">
   This paper explores We present VLG-BERT, a novel LLM model conceived to improve language meaning encoding. VLG-BERT provides deeper insights about meaning encoding in Large Language Models (LLMs) by focusing 
   on linguistic and real-world semantics. It uses syntactic dependencies as a form of ground truth to supervise the learning process of word representations. VLG-BERT incorporates visual latent representations 
   from pre-trained vision models and their corresponding labels. A vocabulary of 10k tokens corresponding to so-called concrete words is built by extending the set of ImageNet labels. The extension is based on 
   synonyms, hyponyms, and hypernyms from WordNet. A lookup table for this vocabulary is then used to initialize the embedding matrix during training, rather than random initialization. This multimodal grounding 
   provides a stronger semantic foundation for encoding the meaning of words. Its architecture aligns seamlessly with foundational theories from across the cognitive sciences. The integration of visual and 
   linguistic grounding makes VLG-BERT consistent with many cognitive theories. Our approach contributes to the ongoing effort to create models that bridge the gap between language and vision, making them more 
   aligned with how humans understand and interpret the world. Experiments on text classification have shown excellent results compared to BERT Base.
   </div>

2. **[Syntax-Constraint-Aware SCABERT: Syntactic Knowledge as a Ground Truth Supervisor of Attention Mechanism via Augmented Lagrange Multipliers](https://icict.co.uk/publication.php)** 
   Published in [**ICICT 2025** - **Springer**], Feb 19 2025, **London,UK**.
   <div align="justify">
   This paper introduces Syntax-Constraint-Aware BERT, a novel variant of BERT designed to inject syntactic knowledge into the attention mechanism using augmented Lagrange multipliers. The model employs syntactic 
   dependencies as a form of ground truth to supervise the learning process of word representation, thereby ensuring that syntactic structure exerts an influence on the model's word representations. The 
   application of augmented Lagrangian optimisation enables the imposition of constraints on the attention mechanism, thereby facilitating the learning of syntactic relationships. This approach involves the 
   augmentation of the standard BERT architecture through the modification of the prediction layer. The objective is to predict an adjacency matrix that encodes words' syntactic relationships in place of the 
   masked tokens. The results of our experiments demonstrate that the injection of syntactic knowledge leads to improved performance in comparison to BERT in terms of training time and also on AG News text 
   classification as a downstream task. By combining the flexibility of deep learning with structured linguistic knowledge, we introduce a merge between bottom-up and top-down approaches. Furthermore, Syntax- 
   Constraint-Aware BERT enhances the interpretability and performance of Transformer-based models.
   </div>
3. **[LingBERT, Linguistic Knowledge Injection into Attention Mechanism based on a Hybrid Masking Strategy](https://www.icmla-conference.org/icmla24/)** 
   Published in [**ICMLA 2024** - **IEEE**], Dec 18 2024, **Miami,USA**.
   <div align="justify">
   In this paper, we propose lingBERT, a transformers based language model. We present two architectures of lingBERT based on hybrid masking strategy. Both models are inspired by BERT base. Our model introduces 
   linguistic knowledge (syntactic dependencies ) injection into attention mechanisms. Models like BERT employ random masking of tokens during training, which can lead to inefficiencies in capturing syntactic 
   and semantic dependencies. To address this, our method uses two masking strategies. The first one masks words with syntactic dependencies. The second one uses a low percentage of randomly masked words. 
   Resulted tokens from both strategies are then handed over tow proposed architectures of lingBERT. This strategy ensures that linguistic relationships are preserved and learned more effectively. Additionally, 
   we maintain a low  randomness ratio of masked tokens to avoid overfitting and enhance the model generalization. Through comprehensive experiments and evaluations, our approach demonstrates significant 
   improvements in capturing context, leading to better performance across various NLP tasks. Furthermore, our approach affords an interpretability about our model inner workings throughout the learning process. 
   This work offers a new direction toward knowledge injection into attention-mechanism based models, leading to advancing the capabilities of language understanding systems.
   </div>
4. **[Reinforcement of BERT with Dependency-Parsing based Attention Mask](https://iccci.pwr.edu.pl/2025/bibliometrics.php)** 
   Published in [**ICCCI 2022** - **Springer**], Aug 28 2022, **Hammemet,TUNISIA**.
   <div align="justify">
   Dot-Product based attention mechanism is among recent attention mechanisms. It showed an outstanding performance with BERT. In this paper, we propose a dependency-parsing mask to reinforce the padding mask, 
   at the multi-head attention units. Padding mask, is already used to filter padding positions. The proposed mask, aims to improve BERT attention filter. The conducted experiments, show that BERT performs 
   better with the proposed mask.
   </div>
---

## Labs Affiliation :

 - ### [Applied AI Lab LI2A](https://oraprdnt.uqtr.uquebec.ca/portail/gscw031?owa_no_site=6345&owa_no_fiche=3)
 - ### [Cognitive Information Analysis Lab](https://www.lanci.uqam.ca/)

## Conferences NLP Special Tracks Reviewer :
-	IEEE  International Conference on Machine Learning and applications ICMLA (Special Track on NLP).
-	Springer International Computational Collective Intelligence ICCCI (Special Track on NLP)

